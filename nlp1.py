# Install NLTK
!pip install -q nltk

# Import libraries
import nltk
from nltk.tokenize import WhitespaceTokenizer, WordPunctTokenizer, TreebankWordTokenizer, TweetTokenizer, MWETokenizer
from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer

# Download data
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')

# Sample sentence
sentence = "Success usually comes to those who are too busy to be looking for it."

# Tokenizers
print("ðŸ”¹ Whitespace Tokenizer:")
print(WhitespaceTokenizer().tokenize(sentence), '\n')

print("ðŸ”¹ WordPunct Tokenizer (Punctuation-based):")
print(WordPunctTokenizer().tokenize(sentence), '\n')

print("ðŸ”¹ Treebank Word Tokenizer:")
tokens = TreebankWordTokenizer().tokenize(sentence)  # âœ… Use this for rest of processing
print(tokens, '\n')

print("ðŸ”¹ Tweet Tokenizer:")
print(TweetTokenizer().tokenize(sentence), '\n')

print("ðŸ”¹ MWE Tokenizer (combining 'too busy'):")
mwe = MWETokenizer([('too', 'busy')])
print(mwe.tokenize(sentence.split()), '\n')

# Stemming
porter = PorterStemmer()
snowball = SnowballStemmer("english")

print("ðŸ”¹ Porter Stemmer:")
print([porter.stem(word) for word in tokens], '\n')

print("ðŸ”¹ Snowball Stemmer:")
print([snowball.stem(word) for word in tokens], '\n')

# Lemmatization
lemmatizer = WordNetLemmatizer()
print("ðŸ”¹ Lemmatization (WordNet):")
print([lemmatizer.lemmatize(word) for word in tokens])
